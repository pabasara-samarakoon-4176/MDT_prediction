{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuvq0FM2KtOOTyFUIECdAZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fyNfOx6Lgf3F","executionInfo":{"status":"ok","timestamp":1761482029562,"user_tz":-330,"elapsed":35491,"user":{"displayName":"pabasara samarakoon","userId":"06037289092954073944"}},"outputId":"45b649a7-85cf-406a-a8c5-3bbd90308d1f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from getpass import getpass\n","\n","# Prompt for token securely\n","token = getpass('Enter your GitHub personal access token: ')\n","\n","username = \"pabasara-samarakoon-4176\"\n","\n","# Clone using token authentication\n","!git clone https://{username}:{token}@github.com/pabasara-samarakoon-4176/MDT_prediction.git"],"metadata":{"id":"5KAU9vF6ggbK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"-DpfGJyBUp3p","executionInfo":{"status":"ok","timestamp":1761478985705,"user_tz":-330,"elapsed":6,"user":{"displayName":"pabasara samarakoon","userId":"06037289092954073944"}}},"outputs":[],"source":["# This code creates the base model.\n","# Train on the initial data and optimise the number of attention heads and layers.\n","# Should use a GPU"]},{"cell_type":"code","source":["import pandas as pd\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"JMn5f9s-U8Jr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = '/content/drive/MyDrive/Final_year_project/datasets/cell_sites_v3.csv'\n","df = pd.read_csv(filename)\n","df.head()"],"metadata":{"id":"FSXXMgMpU-v8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","def undersample_by_percentile(\n","    df,\n","    cell_col=\"Cell_ID\",\n","    target_col=\"RSRP\",\n","    n_bins=5,\n","    random_state=42\n","):\n","    \"\"\"\n","    Undersample per cell based on percentile bins of the target column.\n","    Ensures more balanced representation of signal strengths.\n","\n","    Args:\n","        df (pd.DataFrame): Input dataframe.\n","        cell_col (str): Column name for cell IDs.\n","        target_col (str): Column with target values (RSRP/RSRQ).\n","        n_bins (int): Number of percentile bins.\n","        random_state (int): Random seed.\n","\n","    Returns:\n","        pd.DataFrame: Undersampled dataframe.\n","    \"\"\"\n","    sampled_dfs = []\n","    quantiles = np.linspace(0, 1, n_bins + 1)\n","    bins = df[target_col].quantile(quantiles).values\n","\n","    for cell_id, group in df.groupby(cell_col):\n","        if group.empty:\n","            continue\n","\n","        # Create bins based on quantiles\n","        group['bin'] = pd.cut(group[target_col], bins=bins, include_lowest=True, duplicates='drop')\n","\n","        # Balance across bins by undersampling\n","        min_size = group['bin'].value_counts().min()\n","        sampled = group.groupby('bin').apply(\n","            lambda x: x.sample(n=min_size, random_state=random_state)\n","        ).reset_index(drop=True)\n","\n","        sampled_dfs.append(sampled.drop(columns=['bin']))\n","\n","    return pd.concat(sampled_dfs).reset_index(drop=True)"],"metadata":{"id":"wpQTMFfgVA7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = undersample_by_percentile(\n","    df,\n","    cell_col='Cell_ID',\n","    target_col='RSRP'\n",")"],"metadata":{"id":"SYoE8u4zVNPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import geohash2\n","\n","def geohash_to_latlon_center(gh):\n","    lat, lon, _, _ = geohash2.decode_exactly(gh)\n","    return lat, lon"],"metadata":{"id":"dMv-SkJ-VZvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df['lat'], train_df['lon'] = zip(*train_df['Geohash'].map(geohash_to_latlon_center))"],"metadata":{"id":"03jVeU0XVaBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from pyproj import Proj, Transformer\n","\n","def latlon_to_cartesian(df, site_lat, site_lon, lat_col, lon_col):\n","    \"\"\"\n","    Convert lat/lon positions to Cartesian x, y relative to site location.\n","    \"\"\"\n","    # Define a local projection centered at the site\n","    proj = Proj(proj='aeqd', lat_0=site_lat, lon_0=site_lon, datum='WGS84')\n","    transformer = Transformer.from_proj(\"epsg:4326\", proj, always_xy=True)\n","\n","    # Apply transformation\n","    xs, ys = transformer.transform(df[lon_col].values, df[lat_col].values)\n","\n","    df['x'] = xs\n","    df['y'] = ys\n","    return df"],"metadata":{"id":"JrgHbnS9VciN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_df_cartesian = pd.DataFrame()\n","\n","for cell_id, group in tqdm(train_df.groupby(\"Cell_ID\")):\n","    site_lat = group[\"Site_latitude\"].iloc[0]\n","    site_lon = group[\"Site_longitude\"].iloc[0]\n","\n","    group_cartesian = latlon_to_cartesian(group, site_lat, site_lon,\n","                                          lat_col=\"lat\", lon_col=\"lon\")\n","    train_df_cartesian = pd.concat([train_df_cartesian, group_cartesian])"],"metadata":{"id":"93GCdunqVeln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = [\n","    'EARFCN_DL',\n","    'antenna_height',\n","    'azimuth',\n","    'tilt',\n","    'building_count',\n","    'total_road_length',\n","    'elevation',\n","    'NDVI',\n","    'population_density'\n","]\n","\n","positional_encoding = ['x', 'y']\n","\n","target = ['RSRP', 'RSRQ']"],"metadata":{"id":"mVFoOXrtVjb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch\n","\n","def prepare_sequence_tensor(df, seq_len, feature_cols, pos_cols, target_cols):\n","    N = (len(df) // seq_len) * seq_len\n","    if N == 0:\n","        return None, None, None\n","    df = df.iloc[:N]\n","\n","    num_seq = N // seq_len\n","    input_tensor = torch.tensor(df[feature_cols].values, dtype=torch.float32).view(num_seq, seq_len, -1)\n","    pos_tensor = torch.tensor(df[pos_cols].values, dtype=torch.float32).view(num_seq, seq_len, -1)\n","    target_tensor = torch.tensor(df[target_cols].values, dtype=torch.float32).view(num_seq, seq_len, -1)\n","    return input_tensor, pos_tensor, target_tensor"],"metadata":{"id":"pIyG_6C1Vlp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","import numpy as np\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n","        self.out_proj = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x):\n","        B, S, D = x.shape\n","        qkv = self.qkv_proj(x).reshape(B, S, self.num_heads, 3 * self.d_k).transpose(1, 2)\n","        Q, K, V = qkv.chunk(3, dim=-1)\n","        scores = Q @ K.transpose(-2, -1) / np.sqrt(self.d_k)\n","        attn = F.softmax(scores, dim=-1)\n","        context = attn @ V\n","        context = context.transpose(1, 2).reshape(B, S, D)\n","        return self.out_proj(context)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout=0.5):\n","        super().__init__()\n","        self.attn = MultiHeadAttention(d_model, num_heads)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.ReLU(),\n","            nn.Linear(d_ff, d_model)\n","        )\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.norm1(x + self.dropout(self.attn(x)))\n","        return self.norm2(x + self.dropout(self.ff(x)))\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, output_dim, d_model=128, num_heads=4, num_layers=6, d_ff=256):\n","        super().__init__()\n","        self.input_proj = nn.Linear(input_dim, d_model)\n","        self.pos_proj = nn.Linear(2, d_model)\n","        self.layers = nn.ModuleList([\n","            TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)\n","        ])\n","        self.output_layer = nn.Linear(d_model, output_dim)\n","\n","    def forward(self, x, pos):\n","        x = self.input_proj(x) + self.pos_proj(pos)\n","        for layer in self.layers:\n","            x = layer(x)\n","        return self.output_layer(x)"],"metadata":{"id":"xUgHSsEcVnWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequence_length = 256\n","batch_size = 32"],"metadata":{"id":"wIXA1h61VpAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"c_rZqayAVrr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_scalers = {}\n","for col in features:\n","    scaler = StandardScaler()\n","    train_df[col] = scaler.fit_transform(train_df[[col]])\n","    feature_scalers[col] = scaler\n","\n","target_scalers = {}\n","for col in target:\n","    scaler = StandardScaler()\n","    train_df[col] = scaler.fit_transform(train_df[[col]])\n","    target_scalers[col] = scaler"],"metadata":{"id":"bbAHaP-RVuGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Change the model saving locations in the following cell."],"metadata":{"id":"7jaq15ogWAbT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# ✅ Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ✅ Convert into tensors\n","X, pos, y = prepare_sequence_tensor(\n","    train_df, sequence_length, features, positional_encoding, target\n",")\n","\n","# ✅ Dataset and Train/Val Split (80/20)\n","dataset = TensorDataset(X, pos, y)\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# ✅ Model, Loss, Optimizer, Scheduler\n","model = TransformerModel(input_dim=len(features), output_dim=len(target)).to(device)\n","if torch.cuda.device_count() > 1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs\")\n","    model = nn.DataParallel(model)\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","\n","# ✅ Training loop with Early Stopping\n","epochs = 100\n","patience = 10  # stop if no improvement for 10 epochs\n","best_val_loss = float(\"inf\")\n","early_stop_counter = 0\n","train_losses, val_losses = [], []\n","\n","for epoch in range(1, epochs + 1):\n","    # ---- Training ----\n","    model.train()\n","    total_train_loss = 0\n","    for xb, pb, yb in train_loader:\n","        xb, pb, yb = xb.to(device), pb.to(device), yb.to(device)\n","        optimizer.zero_grad()\n","        preds = model(xb, pb)\n","        loss = criterion(preds, yb)\n","        loss.backward()\n","        optimizer.step()\n","        total_train_loss += loss.item()\n","    avg_train_loss = total_train_loss / len(train_loader)\n","\n","    # ---- Validation ----\n","    model.eval()\n","    total_val_loss = 0\n","    with torch.no_grad():\n","        for xb, pb, yb in val_loader:\n","            xb, pb, yb = xb.to(device), pb.to(device), yb.to(device)\n","            preds = model(xb, pb)\n","            loss = criterion(preds, yb)\n","            total_val_loss += loss.item()\n","    avg_val_loss = total_val_loss / len(val_loader)\n","\n","    # Track losses\n","    train_losses.append(avg_train_loss)\n","    val_losses.append(avg_val_loss)\n","\n","    print(f\"Epoch [{epoch}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n","\n","    # ---- LR Scheduler ----\n","    scheduler.step(avg_val_loss)\n","\n","    # ---- Save Best Model ----\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        early_stop_counter = 0\n","        torch.save({\n","            \"epoch\": epoch,\n","            \"model_state_dict\": model.state_dict(),\n","            \"optimizer_state_dict\": optimizer.state_dict(),\n","            \"train_loss\": avg_train_loss,\n","            \"val_loss\": avg_val_loss,\n","        }, \"/content/drive/MyDrive/Final_year_project/models/1017v4/best_model.pt\")\n","        print(f\"✅ Saved Best Model at Epoch {epoch} with Val Loss {avg_val_loss:.4f}\")\n","    else:\n","        early_stop_counter += 1\n","        print(f\"⏳ EarlyStopping counter: {early_stop_counter}/{patience}\")\n","\n","    # ---- Early Stopping ----\n","    if early_stop_counter >= patience:\n","        print(\"⚠️ Early stopping triggered!\")\n","        break\n","\n","# ✅ Save Final Model\n","torch.save({\n","    \"epoch\": epoch,\n","    \"model_state_dict\": model.state_dict(),\n","    \"optimizer_state_dict\": optimizer.state_dict(),\n","    \"train_losses\": train_losses,\n","    \"val_losses\": val_losses\n","}, \"/content/drive/MyDrive/Final_year_project/models/1017v4/final_model.pt\")\n","\n","print(\"🎉 Training complete. Best model and final model saved.\")"],"metadata":{"id":"uYLn_PtZV8n6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CYBmXywzWG-6"},"execution_count":null,"outputs":[]}]}