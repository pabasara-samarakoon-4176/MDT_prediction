{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPO2kR1JmZOzdePXot8Zz8w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o640JCT9gAjP","executionInfo":{"status":"ok","timestamp":1761481888846,"user_tz":-330,"elapsed":25865,"user":{"displayName":"pabasara samarakoon","userId":"06037289092954073944"}},"outputId":"7bb0d7b7-2eca-4007-ed2a-a5dd4a60d5f6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from getpass import getpass\n","\n","# Prompt for token securely\n","token = getpass('Enter your GitHub personal access token: ')\n","\n","username = \"pabasara-samarakoon-4176\"\n","\n","# Clone using token authentication\n","!git clone https://{username}:{token}@github.com/pabasara-samarakoon-4176/MDT_prediction.git"],"metadata":{"id":"c9tMdX1cgHF5","executionInfo":{"status":"ok","timestamp":1761481895391,"user_tz":-330,"elapsed":3595,"user":{"displayName":"pabasara samarakoon","userId":"06037289092954073944"}},"outputId":"00b1ad10-29c0-404e-eb0a-a0931de0ed51","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter your GitHub personal access token: ··········\n","Cloning into 'MDT_prediction'...\n","remote: Enumerating objects: 163, done.\u001b[K\n","remote: Counting objects: 100% (163/163), done.\u001b[K\n","remote: Compressing objects: 100% (147/147), done.\u001b[K\n","remote: Total 163 (delta 77), reused 50 (delta 14), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (163/163), 5.13 MiB | 9.04 MiB/s, done.\n","Resolving deltas: 100% (77/77), done.\n"]}]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/Colab\\ Notebooks/Inference_and_validation.ipyn.ipynb /content/MDT_prediction/"],"metadata":{"id":"KZuWf-Q_gL2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This code is for infer and validate the fine-tuned model.\n","# Load a dataset consists of antenna features: Cell_ID, Site_ID, Site_latitude, Site_longitude, tilt, azimuth, antenna_height, EARFCN_DL\n","# Use a GPU"],"metadata":{"id":"4YN1NJ4RZHmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTe1G4H1YwPQ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["test_df_antenna = pd.read_csv('test_df.csv')"],"metadata":{"id":"uLKNtqwvY8Mb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import geohash2\n","from geopy.distance import distance\n","\n","# === Your given max_range function ===\n","def max_range(earfcn_dl, antenna_height):\n","    if earfcn_dl == 525:\n","        base_range = 5000\n","    elif earfcn_dl == 1650:\n","        base_range = 3000\n","    else:\n","        base_range = 1500\n","    return base_range + (antenna_height * 10)"],"metadata":{"id":"9AxkZRCdZrKH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import geohash2\n","import pandas as pd\n","import numpy as np\n","import folium\n","from shapely.geometry import Point, Polygon\n","from shapely.ops import unary_union"],"metadata":{"id":"Jsn1ah9zZwRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_sector_polygon(lat, lon, azimuth, radius_m, beamwidth=65, num_points=100):\n","    \"\"\"Return a shapely polygon representing the coverage sector.\"\"\"\n","    # Convert degrees to radians\n","    az_rad = math.radians(azimuth)\n","    half_bw = math.radians(beamwidth / 2)\n","\n","    # Earth radius (meters)\n","    R = 6371000\n","\n","    # Compute arc boundary points\n","    lats, lons = [], []\n","    for theta in np.linspace(az_rad - half_bw, az_rad + half_bw, num_points):\n","        d = radius_m\n","        lat2 = math.degrees(math.asin(math.sin(math.radians(lat)) * math.cos(d/R) +\n","                                      math.cos(math.radians(lat)) * math.sin(d/R) * math.cos(theta)))\n","        lon2 = lon + math.degrees(math.atan2(math.sin(theta) * math.sin(d/R) * math.cos(math.radians(lat)),\n","                                             math.cos(d/R) - math.sin(math.radians(lat)) * math.sin(math.radians(lat2))))\n","        lats.append(lat2)\n","        lons.append(lon2)\n","\n","    # Build polygon (center + arc points)\n","    points = [(lat, lon)] + list(zip(lats, lons)) + [(lat, lon)]\n","    return Polygon([(p[1], p[0]) for p in points])  # lon, lat order for shapely"],"metadata":{"id":"i0lqGYGMZxYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def polygon_to_geohash8(polygon, precision=8, step_m=50):\n","    \"\"\"Fill the polygon with geohash8 tiles spaced roughly step_m apart.\"\"\"\n","    minx, miny, maxx, maxy = polygon.bounds\n","\n","    # Create grid points inside bounding box\n","    lats = np.arange(miny, maxy, step_m / 111320)  # 1 deg ≈ 111.32 km\n","    lons = np.arange(minx, maxx, step_m / (111320 * math.cos(math.radians((miny+maxy)/2))))\n","\n","    geohashes = set()\n","    for lat in lats:\n","        for lon in lons:\n","            p = Point(lon, lat)\n","            if polygon.contains(p):\n","                geohashes.add(geohash2.encode(lat, lon, precision))\n","\n","    return list(geohashes)"],"metadata":{"id":"fVhdaeFTZzNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["geo_rows = []"],"metadata":{"id":"28hg5MHpZ06U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, row in test_df_antenna.iterrows():\n","    site_lat = row['Site_latitude']\n","    site_lon = row['Site_longitude']\n","    azimuth = row['azimuth']\n","    tilt = row['tilt']\n","    h = row['antenna_height']\n","    cell_id = row['Cell_ID']\n","    site_id = row['Site_ID']\n","    earfcn = row['EARFCN_DL']\n","    r_max = max_range(earfcn, h)\n","    poly = create_sector_polygon(site_lat, site_lon, azimuth, r_max)\n","    geohashes = polygon_to_geohash8(poly)\n","\n","    for gh in geohashes:\n","        geo_rows.append({\n","            'Geohash': gh,\n","            'Cell_ID': cell_id,\n","            'Site_ID': site_id,\n","            'Site_Latitude': site_lat,\n","            'Site_Longitude': site_lon,\n","            'azimuth': azimuth,\n","            'tilt': tilt,\n","            'antenna_height': h,\n","            'EARFCN_DL': earfcn\n","        })"],"metadata":{"id":"0ESQHTu6Z2lc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_regen = pd.DataFrame(geo_rows)"],"metadata":{"id":"WtGjzjkLZ9zv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import geopandas as gpd\n","import numpy as np\n","import geohash2\n","import rasterio\n","from rasterio.transform import rowcol\n","from scipy.spatial import cKDTree\n","from tqdm import tqdm\n","\n","def enrich_with_environmental_features(prediction_df,\n","                                       building_data_file,\n","                                       road_data_file,\n","                                       elevation_raster_file,\n","                                       ndvi_files,\n","                                       population_file):\n","    \"\"\"\n","    Enrich prediction_df with building_count, total_road_length,\n","    elevation, NDVI, and population_density features.\n","\n","    Args:\n","        prediction_df (pd.DataFrame): must contain 'Geohash'\n","        building_data_file (str): path to OSM buildings shapefile\n","        road_data_file (str): path to OSM roads shapefile\n","        elevation_raster_file (str): path to elevation .tif\n","        ndvi_files (list): list of NDVI raster .tif files\n","        population_file (str): path to population density CSV\n","\n","    Returns:\n","        pd.DataFrame: enriched prediction_df\n","    \"\"\"\n","    tqdm.pandas()\n","\n","    # --- Buildings ---\n","    building_data = gpd.read_file(building_data_file).to_crs(\"EPSG:4326\")\n","    building_data[\"Centroid\"] = building_data.geometry.centroid\n","    building_data['Geohash'] = building_data[\"Centroid\"].progress_apply(\n","        lambda pt: geohash2.encode(pt.y, pt.x, precision=8))\n","    building_counts = building_data.groupby('Geohash').size().reset_index(name='building_count')\n","    prediction_df = prediction_df.merge(building_counts, on='Geohash', how='left')\n","    prediction_df['building_count'] = prediction_df['building_count'].fillna(0).astype(int)\n","\n","    # --- Roads ---\n","    road_data = gpd.read_file(road_data_file)\n","    road_data = road_data.set_crs(\"EPSG:4326\", allow_override=True).to_crs(\"EPSG:5234\")\n","    road_data['length_m'] = road_data.geometry.length\n","    road_data['centroid'] = road_data.geometry.centroid\n","    centroids_geo = gpd.GeoSeries(road_data[\"centroid\"], crs=\"EPSG:5234\").to_crs(\"EPSG:4326\")\n","    road_data[\"Geohash\"] = centroids_geo.apply(lambda pt: geohash2.encode(pt.y, pt.x, precision=8))\n","    road_grouped = road_data.groupby(\"Geohash\").agg(total_road_length=(\"length_m\", \"sum\")).reset_index()\n","    prediction_df = prediction_df.merge(road_grouped, on=\"Geohash\", how=\"left\")\n","    prediction_df['total_road_length'] = prediction_df['total_road_length'].fillna(0).astype(int)\n","\n","    # --- Elevation ---\n","    def geohash_to_latlon_center(gh):\n","        lat, lon, _, _ = geohash2.decode_exactly(gh)\n","        return lat, lon\n","    prediction_df['lat'], prediction_df['lon'] = zip(*prediction_df['Geohash'].map(geohash_to_latlon_center))\n","\n","    def get_raster_value(raster, lon, lat):\n","        try:\n","            coords = [(lon, lat)]\n","            for val in raster.sample(coords):\n","                return val[0] if val[0] != raster.nodata else np.nan\n","        except:\n","            return np.nan\n","\n","    with rasterio.open(elevation_raster_file) as elev_src:\n","        prediction_df['elevation'] = prediction_df.progress_apply(\n","            lambda row: get_raster_value(elev_src, row['lon'], row['lat']), axis=1\n","        )\n","\n","    # --- NDVI ---\n","    rasters = []\n","    for file in ndvi_files:\n","        src = rasterio.open(file)\n","        data = src.read(1).astype(np.float32) / 65535.0\n","        rasters.append((src, data))\n","\n","    def get_ndvi(lat, lon):\n","        for src, ndvi_data in rasters:\n","            try:\n","                row, col = rowcol(src.transform, lon, lat)\n","                if (0 <= row < ndvi_data.shape[0]) and (0 <= col < ndvi_data.shape[1]):\n","                    return float(ndvi_data[row, col])\n","            except:\n","                continue\n","        return np.nan\n","\n","    prediction_df[\"NDVI\"] = prediction_df.progress_apply(\n","        lambda row: get_ndvi(row[\"lat\"], row[\"lon\"]), axis=1\n","    )\n","\n","    # --- Population density ---\n","    df_pop = pd.read_csv(population_file)\n","    gdf = gpd.GeoDataFrame(prediction_df,\n","                           geometry=gpd.points_from_xy(prediction_df.lon, prediction_df.lat),\n","                           crs=\"EPSG:4326\").to_crs(\"EPSG:5234\")\n","    gdf_pop = gpd.GeoDataFrame(df_pop,\n","                               geometry=gpd.points_from_xy(df_pop.X, df_pop.Y),\n","                               crs=\"EPSG:4326\").to_crs(\"EPSG:5234\")\n","    points = np.array(list(zip(gdf.geometry.x, gdf.geometry.y)))\n","    pop_points = np.array(list(zip(gdf_pop.geometry.x, gdf_pop.geometry.y)))\n","    population_tree = cKDTree(pop_points)\n","    distances, indices = population_tree.query(points, distance_upper_bound=1000)\n","\n","    prediction_df['population_density'] = [gdf_pop.iloc[i]['Z'] if i < len(gdf_pop) else 0\n","                                           for i in tqdm(indices)]\n","\n","    # --- Cleanup ---\n","    return prediction_df.drop(columns=['lat', 'lon'], errors='ignore')"],"metadata":{"id":"nIApCFRFZ-Ms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_regen = enrich_with_environmental_features(\n","    test_df_regen,\n","    building_data_file=\"/content/drive/MyDrive/Final_year_project/building/gis_osm_buildings_a_free_1.shp\",\n","    road_data_file=\"/content/drive/MyDrive/Final_year_project/roads/gis_osm_roads_free_1.shp\",\n","    elevation_raster_file=\"/content/drive/MyDrive/Final_year_project/terrain/elevation.tif\",\n","    ndvi_files=[\n","        \"/content/drive/MyDrive/Final_year_project/vegetation/ndvi_west.tiff\",\n","        \"/content/drive/MyDrive/Final_year_project/vegetation/ndvi_east.tiff\"\n","    ],\n","    population_file=\"/content/drive/MyDrive/Final_year_project/population/population_density.csv\"\n",")"],"metadata":{"id":"4qCnewSaaAIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import geohash2\n","import pandas as pd\n","\n","def add_geohash_centroids(prediction_df, geohash_col=\"Geohash\"):\n","    \"\"\"\n","    Adds centroid latitude and longitude of geohashes in prediction_df.\n","\n","    Args:\n","        prediction_df (pd.DataFrame): must contain a 'Geohash' column\n","        geohash_col (str): name of the column with geohash strings (default: \"Geohash\")\n","\n","    Returns:\n","        pd.DataFrame: with two new columns 'lat' and 'lon'\n","    \"\"\"\n","    def geohash_to_latlon_center(gh):\n","        lat, lon, _, _ = geohash2.decode_exactly(gh)\n","        return lat, lon\n","\n","    prediction_df = prediction_df.copy()\n","    prediction_df['lat'], prediction_df['lon'] = zip(*prediction_df[geohash_col].map(geohash_to_latlon_center))\n","    return prediction_df"],"metadata":{"id":"FH6Uzs0daBgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_regen = add_geohash_centroids(test_df_regen)"],"metadata":{"id":"1MWdV7lKaFwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_regen = test_df_regen.rename(columns={\n","    'Site_Latitude': 'Site_latitude',\n","    'Site_Longitude': 'Site_longitude'\n","})"],"metadata":{"id":"RhOt4J0KaGPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","test_df_cartesian = pd.DataFrame()\n","\n","for cell_id, group in tqdm(test_df_regen.groupby(\"Cell_ID\")):\n","    site_lat = group[\"Site_latitude\"].iloc[0]\n","    site_lon = group[\"Site_longitude\"].iloc[0]\n","\n","    group_cartesian = latlon_to_cartesian(group, site_lat, site_lon,\n","                                          lat_col=\"lat\", lon_col=\"lon\")\n","    test_df_cartesian = pd.concat([test_df_cartesian, group_cartesian])"],"metadata":{"id":"ksq-aZ9YaIUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_rot_ind = pd.DataFrame()"],"metadata":{"id":"ehl4cy0RaJ5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for cell_id, group in test_df_cartesian.groupby(\"Cell_ID\"):\n","    print(f\"Cell ID: {cell_id}\\n\")\n","    group = group.copy()\n","    group[\"angle_deg\"] = np.degrees(np.arctan2(group[\"y\"], group[\"x\"])) % 360\n","    group[\"folded_angle\"] = group[\"angle_deg\"].progress_apply(mirror_azimuth)\n","    group[\"rotation_angle_deg\"] = -(group[\"angle_deg\"] - group[\"folded_angle\"])\n","    group[\"rotation_angle_rad\"] = np.deg2rad(group[\"rotation_angle_deg\"])\n","\n","    cos_a = np.cos(group[\"rotation_angle_rad\"])\n","    sin_a = np.sin(group[\"rotation_angle_rad\"])\n","    x_new = group[\"x\"] * cos_a - group[\"y\"] * sin_a\n","    y_new = group[\"x\"] * sin_a + group[\"y\"] * cos_a\n","\n","    group[\"x\"] = x_new\n","    group[\"y\"] = y_new\n","\n","    test_df_rot_ind = pd.concat([test_df_rot_ind, group])"],"metadata":{"id":"eKINkiUSaNOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = [\n","    'EARFCN_DL',\n","    'antenna_height',\n","    'azimuth',\n","    'tilt',\n","    'building_count',\n","    'total_road_length',\n","    'elevation',\n","    'NDVI',\n","    'population_density'\n","]\n","\n","positional_encoding = ['x', 'y']\n","\n","target = ['RSRP', 'RSRQ']"],"metadata":{"id":"aI_51qNsamq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_scalers = {}\n","for col in features:\n","    scaler = StandardScaler()\n","    train_df[col] = scaler.fit_transform(train_df[[col]])\n","    feature_scalers[col] = scaler\n","\n","target_scalers = {}\n","for col in target:\n","    scaler = StandardScaler()\n","    train_df[col] = scaler.fit_transform(train_df[[col]])\n","    target_scalers[col] = scaler"],"metadata":{"id":"mCsrQcJ3ahy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in features:\n","    scaler = feature_scalers[col]\n","    test_df_processed[col] = scaler.transform(test_df_processed[[col]])"],"metadata":{"id":"ZPVtQtYLaOwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_prediction_tensor(df, seq_len, feature_cols, pos_cols):\n","    N = (len(df) // seq_len) * seq_len\n","    if N == 0:\n","        raise ValueError(\"Not enough rows in prediction_df_processed for one full sequence.\")\n","\n","    df = df.iloc[:N]\n","    num_seq = N // seq_len\n","\n","    X_tensor = torch.tensor(df[feature_cols].values, dtype=torch.float32).view(num_seq, seq_len, -1)\n","    pos_tensor = torch.tensor(df[pos_cols].values, dtype=torch.float32).view(num_seq, seq_len, -1)\n","    return X_tensor, pos_tensor"],"metadata":{"id":"RRo5Wy_oanOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequence_length = 256\n","X_pred, pos_pred = prepare_prediction_tensor(\n","    test_df_processed,\n","    sequence_length,\n","    features,\n","    ['x', 'y']\n",")"],"metadata":{"id":"LOFW4V1OasVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transformer only (+FFNNs)\n","\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","import numpy as np\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n","        self.out_proj = nn.Linear(d_model, d_model)\n","\n","    def forward(self, x):\n","        B, S, D = x.shape\n","        qkv = self.qkv_proj(x).reshape(B, S, self.num_heads, 3 * self.d_k).transpose(1, 2)\n","        Q, K, V = qkv.chunk(3, dim=-1)\n","        scores = Q @ K.transpose(-2, -1) / np.sqrt(self.d_k)\n","        attn = F.softmax(scores, dim=-1)\n","        context = attn @ V\n","        context = context.transpose(1, 2).reshape(B, S, D)\n","        return self.out_proj(context)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout=0.5):\n","        super().__init__()\n","        self.attn = MultiHeadAttention(d_model, num_heads)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.ReLU(),\n","            nn.Linear(d_ff, d_model)\n","        )\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.norm1(x + self.dropout(self.attn(x)))\n","        return self.norm2(x + self.dropout(self.ff(x)))\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, output_dim, d_model=128, num_heads=4, num_layers=6, d_ff=256):\n","        super().__init__()\n","        self.input_proj = nn.Linear(input_dim, d_model)\n","        self.pos_proj = nn.Linear(2, d_model)\n","        self.layers = nn.ModuleList([\n","            TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)\n","        ])\n","        self.output_layer = nn.Linear(d_model, output_dim)\n","\n","    def forward(self, x, pos):\n","        x = self.input_proj(x) + self.pos_proj(pos)\n","        for layer in self.layers:\n","            x = layer(x)\n","        return self.output_layer(x)"],"metadata":{"id":"TKpvmf94ayto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup the fine-tuned model location"],"metadata":{"id":"dZ-jTZO0bqYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = len(features)\n","output_dim = len(target)\n","d_model = 128\n","num_heads = 4\n","num_layers = 6\n","d_ff = 256\n","\n","inf_model = TransformerModel(\n","    input_dim=input_dim,\n","    output_dim=output_dim,\n","    d_model=d_model,\n","    num_heads=num_heads,\n","    num_layers=num_layers,\n","    d_ff=d_ff\n",").to(device)\n","\n","best_model_path = \"/content/best_model.pt\"\n","best_model = torch.load(best_model_path, map_location=device)\n","\n","inf_model.load_state_dict(best_model[\"model_state_dict\"])"],"metadata":{"id":"fpwowP3GazYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inf_model.eval()\n","with torch.no_grad():\n","    X_pred, pos_pred = X_pred.to(device), pos_pred.to(device)\n","    preds = inf_model(X_pred, pos_pred).cpu().numpy()"],"metadata":{"id":"sv3O_fGta8dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_flat = preds.reshape(-1, preds.shape[-1])\n","\n","# Inverse scale each target\n","preds_df = pd.DataFrame(preds_flat, columns=target)\n","for col in target:\n","    scaler = target_scalers[col]\n","    preds_df[col] = scaler.inverse_transform(preds_df[[col]])\n","\n","preds_df.head()"],"metadata":{"id":"wOENlANea-jp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_results = test_df_processed.iloc[:len(preds_df)].copy()\n","test_results[['RSRP', 'RSRQ']] = preds_df[['RSRP', 'RSRQ']]"],"metadata":{"id":"tdxzrVMma-6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in features:\n","    scaler = feature_scalers[col]\n","    test_results[col] = scaler.inverse_transform(test_results[[col]])\n","test_results.head()"],"metadata":{"id":"F_uyRVFBbA4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the locations for Prediction and Actual visualisations"],"metadata":{"id":"GMwjSgkvbEQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import folium\n","import geohash\n","import geohash2\n","\n","# --- Select the site of interest ---\n","site_id = str(input(\"Enter the site ID: \"))   # <-- change this to the Site_ID you want\n","site_df = test_results_processed[test_results_processed['Site_ID'] == site_id].copy()\n","\n","# Extract all cells belonging to that site\n","site_cells = site_df['Cell_ID'].unique()\n","print(f\"Plotting {len(site_cells)} cells for site {site_id}\")\n","\n","# Use precision 8 for geohash aggregation\n","site_df['geohash8'] = site_df['Geohash'].str[:8]\n","\n","# --- Aggregate average RSRP for each geohash ---\n","agg_data = []\n","for gh, group in site_df.groupby('geohash8'):\n","    lat, lon = geohash.decode(gh)\n","    avg_rsrp = group['RSRP'].mean()\n","    count = len(group)\n","    cell_ids = group['Cell_ID'].unique().tolist()\n","    agg_data.append((gh, lat, lon, avg_rsrp, count, cell_ids))\n","\n","agg_df = pd.DataFrame(agg_data, columns=['geohash8', 'avg_lat', 'avg_lon', 'avg_RSRP', 'point_count', 'Cell_IDs'])\n","\n","# --- Color mapping for RSRP ---\n","def rsrp_to_color(rsrp):\n","    if rsrp == 0 or pd.isna(rsrp):\n","        return \"#606060\"  # gray\n","    elif rsrp <= -85:\n","        return \"red\"\n","    elif rsrp <= -75:\n","        return \"orange\"\n","    elif rsrp <= -65:\n","        return \"yellow\"\n","    else:\n","        return \"green\"\n","\n","# --- Initialize map centered on site ---\n","center_lat = site_df['Site_latitude'].mean()\n","center_lon = site_df['Site_longitude'].mean()\n","m_site = folium.Map(location=[center_lat, center_lon], zoom_start=13)\n","\n","# --- Draw rectangles for all geohash cells ---\n","for _, row in agg_df.iterrows():\n","    bbox = geohash2.decode_exactly(row['geohash8'])\n","    lat, lon, lat_err, lon_err = bbox\n","    lat_min, lat_max = lat - lat_err, lat + lat_err\n","    lon_min, lon_max = lon - lon_err, lon + lon_err\n","\n","    color = rsrp_to_color(row['avg_RSRP'])\n","    cell_list = \", \".join(map(str, row['Cell_IDs']))\n","    popup_text = (\n","        f\"<b>Site:</b> {site_id}<br>\"\n","        f\"<b>Cells:</b> {cell_list}<br>\"\n","        f\"Points: {row['point_count']}<br>\"\n","        f\"Avg RSRP: {row['avg_RSRP']:.2f} dBm\"\n","    )\n","\n","    folium.Rectangle(\n","        bounds=[[lat_min, lon_min], [lat_max, lon_max]],\n","        color=color,\n","        fill=True,\n","        fill_opacity=0.6,\n","        popup=popup_text\n","    ).add_to(m_site)\n","\n","# --- Plot site marker ---\n","site_lat = site_df['Site_latitude'].iloc[0]\n","site_lon = site_df['Site_longitude'].iloc[0]\n","folium.Marker(\n","    location=[site_lat, site_lon],\n","    icon=folium.Icon(color='black', icon='signal', prefix='fa'),\n","    popup=f\"Site ID: {site_id}\"\n",").add_to(m_site)\n","\n","# --- Title ---\n","title_html = f'''\n","<div style=\"\n","    position: fixed;\n","    top: 10px;\n","    left: 50%;\n","    transform: translateX(-50%);\n","    z-index: 9999;\n","    font-size: 22px;\n","    font-weight: bold;\n","    background-color: white;\n","    padding: 5px 10px;\n","    border: 2px solid grey;\n","    border-radius: 5px;\n","    box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\n","\">\n","RSRP Distribution Map — Site {site_id} (Prediction)\n","</div>\n","'''\n","m_site.get_root().html.add_child(folium.Element(title_html))\n","\n","# --- Legend ---\n","legend_html = '''\n","<div style=\"position: fixed;\n","            bottom: 50px; left: 50px; width: 200px; height: 160px;\n","            border:2px solid grey; z-index:9999; font-size:14px;\n","            background-color:white;\n","            padding:5px;\">\n","<b>RSRP (dBm)</b><br>\n","<i style=\"background:red;color:red\">....</i>&nbsp; ≤ -85 (Very Weak)<br>\n","<i style=\"background:orange;color:orange\">....</i>&nbsp; -85 to -75 (Weak–Medium)<br>\n","<i style=\"background:yellow;color:yellow\">....</i>&nbsp; -75 to -65 (Medium–Strong)<br>\n","<i style=\"background:green;color:green\">....</i>&nbsp; > -65 (Very Strong)<br>\n","<i style=\"background:#606060;color:#606060\">....</i>&nbsp; Zero/No Data\n","</div>\n","'''\n","m_site.get_root().html.add_child(folium.Element(legend_html))\n","\n","m_site.save(f'site_{site_id}_prediction_map.html')"],"metadata":{"id":"u7qAoEODb3h3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import folium\n","import geohash\n","import geohash2\n","\n","# --- Select the site of interest ---\n","site_id = str(input(\"Enter the site ID: \"))   # <-- change this to the Site_ID you want\n","site_df = test_df[test_df['Site_ID'] == site_id].copy()\n","\n","# Extract all cells belonging to that site\n","site_cells = site_df['Cell_ID'].unique()\n","print(f\"Plotting {len(site_cells)} cells for site {site_id}\")\n","\n","# Use precision 8 for geohash aggregation\n","site_df['geohash8'] = site_df['Geohash'].str[:8]\n","\n","# --- Aggregate average RSRP for each geohash ---\n","agg_data = []\n","for gh, group in site_df.groupby('geohash8'):\n","    lat, lon = geohash.decode(gh)\n","    avg_rsrp = group['RSRP'].mean()\n","    count = len(group)\n","    cell_ids = group['Cell_ID'].unique().tolist()\n","    agg_data.append((gh, lat, lon, avg_rsrp, count, cell_ids))\n","\n","agg_df = pd.DataFrame(agg_data, columns=['geohash8', 'avg_lat', 'avg_lon', 'avg_RSRP', 'point_count', 'Cell_IDs'])\n","\n","# --- Color mapping for RSRP ---\n","def rsrp_to_color(rsrp):\n","    if rsrp == 0 or pd.isna(rsrp):\n","        return \"#606060\"  # gray\n","    elif rsrp <= -85:\n","        return \"red\"\n","    elif rsrp <= -75:\n","        return \"orange\"\n","    elif rsrp <= -65:\n","        return \"yellow\"\n","    else:\n","        return \"green\"\n","\n","# --- Initialize map centered on site ---\n","center_lat = site_df['Site_latitude'].mean()\n","center_lon = site_df['Site_longitude'].mean()\n","m_site = folium.Map(location=[center_lat, center_lon], zoom_start=13)\n","\n","# --- Draw rectangles for all geohash cells ---\n","for _, row in agg_df.iterrows():\n","    bbox = geohash2.decode_exactly(row['geohash8'])\n","    lat, lon, lat_err, lon_err = bbox\n","    lat_min, lat_max = lat - lat_err, lat + lat_err\n","    lon_min, lon_max = lon - lon_err, lon + lon_err\n","\n","    color = rsrp_to_color(row['avg_RSRP'])\n","    cell_list = \", \".join(map(str, row['Cell_IDs']))\n","    popup_text = (\n","        f\"<b>Site:</b> {site_id}<br>\"\n","        f\"<b>Cells:</b> {cell_list}<br>\"\n","        f\"Points: {row['point_count']}<br>\"\n","        f\"Avg RSRP: {row['avg_RSRP']:.2f} dBm\"\n","    )\n","\n","    folium.Rectangle(\n","        bounds=[[lat_min, lon_min], [lat_max, lon_max]],\n","        color=color,\n","        fill=True,\n","        fill_opacity=0.6,\n","        popup=popup_text\n","    ).add_to(m_site)\n","\n","# --- Plot site marker ---\n","site_lat = site_df['Site_latitude'].iloc[0]\n","site_lon = site_df['Site_longitude'].iloc[0]\n","folium.Marker(\n","    location=[site_lat, site_lon],\n","    icon=folium.Icon(color='black', icon='signal', prefix='fa'),\n","    popup=f\"Site ID: {site_id}\"\n",").add_to(m_site)\n","\n","# --- Title ---\n","title_html = f'''\n","<div style=\"\n","    position: fixed;\n","    top: 10px;\n","    left: 50%;\n","    transform: translateX(-50%);\n","    z-index: 9999;\n","    font-size: 22px;\n","    font-weight: bold;\n","    background-color: white;\n","    padding: 5px 10px;\n","    border: 2px solid grey;\n","    border-radius: 5px;\n","    box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\n","\">\n","RSRP Distribution Map — Site {site_id} (Actual)\n","</div>\n","'''\n","m_site.get_root().html.add_child(folium.Element(title_html))\n","\n","# --- Legend ---\n","legend_html = '''\n","<div style=\"position: fixed;\n","            bottom: 50px; left: 50px; width: 200px; height: 160px;\n","            border:2px solid grey; z-index:9999; font-size:14px;\n","            background-color:white;\n","            padding:5px;\">\n","<b>RSRP (dBm)</b><br>\n","<i style=\"background:red;color:red\">....</i>&nbsp; ≤ -85 (Very Weak)<br>\n","<i style=\"background:orange;color:orange\">....</i>&nbsp; -85 to -75 (Weak–Medium)<br>\n","<i style=\"background:yellow;color:yellow\">....</i>&nbsp; -75 to -65 (Medium–Strong)<br>\n","<i style=\"background:green;color:green\">....</i>&nbsp; > -65 (Very Strong)<br>\n","<i style=\"background:#606060;color:#606060\">....</i>&nbsp; Zero/No Data\n","</div>\n","'''\n","m_site.get_root().html.add_child(folium.Element(legend_html))\n","\n","m_site.save(f'site_{site_id}_actual_map.html')"],"metadata":{"id":"HuABg72IcUAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import folium\n","import geohash\n","import matplotlib.colors as mcolors\n","from scipy.stats import pearsonr\n","\n","def compare_rsrp_maps_black_missing(df_pred, df_actual, site_id, metric='RSRP', generate_map=True):\n","    \"\"\"\n","    Compare predicted vs actual RSRP maps for a given Cell_ID, including all Geohash cells.\n","    Missing cells are colored black on the difference map.\n","\n","    Args:\n","        df_pred (pd.DataFrame): Prediction Data Frame\n","        df_actual (pd.DataFrame): Actual Data Frame\n","        generate_map (bool): If True, generates folium difference map.\n","\n","    Returns:\n","        dict: similarity metrics and optionally folium map object.\n","    \"\"\"\n","\n","    df_pred = df_pred[df_pred['Site_ID'] == site_id].copy()\n","    df_actual = df_actual[df_actual['Site_ID'] == site_id].copy()\n","\n","    if df_pred.empty or df_actual.empty:\n","        print(f\"No data found for Cell ID {cell_id}\")\n","        return {}\n","\n","    def aggregate(df):\n","        agg_data = []\n","        for gh, group in df.groupby('Geohash'):\n","            lat, lon = geohash.decode(gh)\n","            avg_val = group[metric].mean()\n","            count = len(group)\n","            agg_data.append((gh, lat, lon, avg_val, count))\n","        return pd.DataFrame(agg_data, columns=['Geohash', 'avg_lat', 'avg_lon', f'avg_{metric}', 'point_count'])\n","\n","    agg_pred = aggregate(df_pred)\n","    agg_actual = aggregate(df_actual)\n","\n","    merged = pd.merge(\n","        agg_pred[['Geohash', 'avg_RSRP', 'avg_lat', 'avg_lon']],\n","        agg_actual[['Geohash', 'avg_RSRP']],\n","        on='Geohash',\n","        how='outer',\n","        suffixes=('_pred', '_actual')\n","    )\n","\n","    merged['avg_lat'] = merged['avg_lat'].combine_first(\n","        merged['Geohash'].apply(lambda gh: geohash.decode(gh)[0])\n","    )\n","    merged['avg_lon'] = merged['avg_lon'].combine_first(\n","        merged['Geohash'].apply(lambda gh: geohash.decode(gh)[1])\n","    )\n","\n","    overlap = merged.dropna(subset=['avg_RSRP_pred', 'avg_RSRP_actual'])\n","    if len(overlap) > 1:\n","        corr, _ = pearsonr(overlap['avg_RSRP_pred'], overlap['avg_RSRP_actual'])\n","        mae = np.mean(np.abs(overlap['avg_RSRP_pred'] - overlap['avg_RSRP_actual']))\n","        rmse = np.sqrt(np.mean((overlap['avg_RSRP_pred'] - overlap['avg_RSRP_actual'])**2))\n","        similarity_score = (corr + 1) / 2\n","        status = \"Exact overlap\"\n","    else:\n","        from sklearn.neighbors import NearestNeighbors\n","\n","        pred_valid = merged.dropna(subset=['avg_lat', 'avg_lon', 'avg_RSRP_pred']).copy()\n","        actual_valid = merged.dropna(subset=['avg_lat', 'avg_lon', 'avg_RSRP_actual']).copy()\n","\n","        if len(pred_valid) > 1 and len(actual_valid) > 1:\n","            nn = NearestNeighbors(n_neighbors=1).fit(actual_valid[['avg_lat', 'avg_lon']])\n","            distances, indices = nn.kneighbors(pred_valid[['avg_lat', 'avg_lon']])\n","\n","            paired_pred_rsrp = pred_valid['avg_RSRP_pred'].values\n","            paired_actual_rsrp = actual_valid.iloc[indices.flatten()]['avg_RSRP_actual'].values\n","\n","            # Now both have equal length\n","            mae = np.mean(np.abs(paired_pred_rsrp - paired_actual_rsrp))\n","            rmse = np.sqrt(np.mean((paired_pred_rsrp - paired_actual_rsrp)**2))\n","            corr = np.corrcoef(paired_pred_rsrp, paired_actual_rsrp)[0, 1]\n","            similarity_score = (corr + 1) / 2\n","            status = \"Spatially approximated (no exact overlap)\"\n","        else:\n","            mae = rmse = np.nan\n","            corr = np.nan\n","            similarity_score = 0.0\n","            status = \"No overlap or insufficient data\"\n","\n","    results = {\n","        'Site ID': site_id,\n","        'MAE': mae,\n","        'RMSE': rmse,\n","        'Pearson Correlation': corr,\n","        'Similarity Score (0-1)': similarity_score,\n","        'Status': status\n","    }\n","\n","    if generate_map:\n","        merged['RSRP_diff'] = merged['avg_RSRP_pred'] - merged['avg_RSRP_actual']\n","        center_lat = merged['avg_lat'].mean()\n","        center_lon = merged['avg_lon'].mean()\n","\n","        vmax = merged['RSRP_diff'].abs().quantile(0.95)\n","        if vmax < 2: vmax = 2\n","        norm_diff = mcolors.TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\n","        cmap_diff = mcolors.LinearSegmentedColormap.from_list(\"\", [\"blue\",\"white\",\"red\"])\n","\n","        m_diff = folium.Map(location=[center_lat, center_lon], zoom_start=15)\n","\n","        for _, row in merged.iterrows():\n","            bbox = geohash.bbox(row['Geohash'])\n","\n","            if pd.isna(row['avg_RSRP_pred']) or pd.isna(row['avg_RSRP_actual']):\n","                color = \"#000000\"  # black for missing cells\n","                popup_text = \"Cell missing in one dataset\"\n","            else:\n","                color = mcolors.to_hex(cmap_diff(norm_diff(row['RSRP_diff'])))\n","                popup_text = f\"RSRP Diff (Pred - Actual): {row['RSRP_diff']:.2f} dBm\"\n","\n","            folium.Rectangle(\n","                bounds=[[bbox['s'], bbox['w']], [bbox['n'], bbox['e']]],\n","                color=color, fill=True, fill_opacity=0.6, popup=popup_text\n","            ).add_to(m_diff)\n","\n","        # --- Add legend ---\n","        legend_html = '''\n","        <div style=\"position: fixed; bottom: 50px; left: 50px; width: 240px;\n","                    background-color: white; border:2px solid grey; z-index:9999;\n","                    padding:10px; font-size:14px;\">\n","        <b>RSRP Difference (Pred - Actual)</b><br>\n","        <i style=\"background:blue;color:blue\">....</i>&nbsp; Under-predicted<br>\n","        <i style=\"background:white;color:white;border:1px solid grey\">....</i>&nbsp; Accurate<br>\n","        <i style=\"background:red;color:red\">....</i>&nbsp; Over-predicted<br>\n","        <i style=\"background:black;color:black\">....</i>&nbsp; Missing cell\n","        </div>\n","        '''\n","        m_diff.get_root().html.add_child(folium.Element(legend_html))\n","        results['Difference Map'] = m_diff\n","\n","    return results"],"metadata":{"id":"4UNdMmFWcYM1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the validation map outputs for each site"],"metadata":{"id":"9V0Bw0GOcaPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["site_id = str(input(\"Enter the Site ID: \"))\n","result = compare_rsrp_maps_black_missing(\n","    df_pred=test_results_processed,\n","    df_actual=test_df,\n","    site_id=site_id,\n","    generate_map=True\n",")"],"metadata":{"id":"x74zoGgJcjwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = []\n","for site_id in test_df['Site_ID'].unique().tolist():\n","    result = compare_rsrp_maps_black_missing(\n","        df_pred=test_results_processed,\n","        df_actual=test_df,\n","        site_id=site_id,\n","        generate_map=False\n","    )\n","    results.append(result)"],"metadata":{"id":"wMmRipaMcmBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df = pd.DataFrame(results)\n","results_df.to_csv('/content/drive/MyDrive/Final_year_project/models/1026v2/results1026v2.csv', index=False)\n","results_df.head()"],"metadata":{"id":"c9qSYWdqcmZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["overall_metrics = {\n","    \"Mean MAE\": results_df[\"MAE\"].mean(),\n","    \"Mean RMSE\": results_df[\"RMSE\"].mean(),\n","    \"Mean Pearson Correlation\": results_df[\"Pearson Correlation\"].mean(),\n","    \"Mean Similarity Score (0-1)\": results_df[\"Similarity Score (0-1)\"].mean(),\n","    \"Total Sites\": len(results_df)\n","}"],"metadata":{"id":"t7WhpxVJcnrW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"=== Overall Evaluation Metrics ===\")\n","for k, v in overall_metrics.items():\n","    print(f\"{k}: {v:.4f}\")"],"metadata":{"id":"x53rrI5Rcp8b"},"execution_count":null,"outputs":[]}]}